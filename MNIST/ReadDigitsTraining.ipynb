{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digits\n",
    "\n",
    "In this notebook we are going to train a neural network to recognize handwritten digits. This is the  \"Hello World\" of deep learning: training a deep learning model to correctly classify hand-written digits.\n",
    "\n",
    "In the previous [notebook](TheMNISTDataSet.ipynb) we downloaded the MNIST dataset, which is a dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.  We will re-use this data (downloaded either to your local hard drive or /transfer) to train a neural network to recognize the digits.\n",
    "\n",
    "We will start by importing the necessary libraries, including our Utils module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils.in_lab()=False\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import struct\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Visualization tools\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import Utils\n",
    "\n",
    "print(f\"{Utils.in_lab()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Support\n",
    "\n",
    "In this notebook we will use the GPU to train our model, we can use the function from our Utils module to check if the GPU is available and set this as the device to use for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = Utils.get_device()\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image classification\n",
    "\n",
    "The approach we are going to take with this example is to load a set of know images and their labels, and train a neural network to learn the relationship between the images and their labels. \n",
    "\n",
    "We will use a neural network and a trial and error system to begin to recognize the patterns in the images. The images are small (28x28 pixels) and the neural network will learn to recognize the patterns in the images that are associated with the digits.\n",
    "\n",
    "We have a set of 60,000 images to train the network and a separate set of 10,000 images to test the network and on each step of the training process we will check the accuracy of the network on the test set.\n",
    "\n",
    "In the previous notebook we created two functions for loading the data and labels so we will use these functions to load the data and labels for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_labels(filename: str) -> np.ndarray:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic, num = struct.unpack(\">II\", f.read(8))\n",
    "        labels = np.fromfile(f, dtype=np.uint8)\n",
    "        if len(labels) != num:\n",
    "            raise ValueError(f\"Expected {num} labels, but got {len(labels)}\")\n",
    "    return labels\n",
    "\n",
    "\n",
    "def load_mnist_images(filename: str) -> np.ndarray:\n",
    "    with open(filename, \"rb\") as f:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        images = np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)\n",
    "        if len(images) != num:\n",
    "            raise ValueError(f\"Expected {num} images, but got {len(images)}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n",
      "5 7\n"
     ]
    }
   ],
   "source": [
    "DATASET_LOCATION = \"\"\n",
    "if Utils.in_lab():\n",
    "    DATASET_LOCATION = \"/transfer/MNIST/\"\n",
    "else:\n",
    "    DATASET_LOCATION = \"./MNIST/\"\n",
    "\n",
    "train_labels = load_mnist_labels(DATASET_LOCATION + \"train-labels-idx1-ubyte\")\n",
    "test_labels = load_mnist_labels(DATASET_LOCATION + \"t10k-labels-idx1-ubyte\")\n",
    "\n",
    "print(len(train_labels), len(test_labels))\n",
    "print(train_labels[0], test_labels[0])\n",
    "\n",
    "# We can now load the images from both the datasets.\n",
    "train_images = load_mnist_images(DATASET_LOCATION + \"train-images-idx3-ubyte\")\n",
    "test_images = load_mnist_images(DATASET_LOCATION + \"t10k-images-idx3-ubyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the images we can define a simple function to display the images. We will use the matplotlib library to display the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "def display_image(image: np.array, label: str) -> None:\n",
    "    plt.figure(figsize=(1, 1))\n",
    "    plt.title(f\"Label : {label}\")\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# We can now display the first image from the training dataset.\n",
    "\n",
    "display_image(train_images[0], train_labels[0])\n",
    "print(type(train_images[0]))\n",
    "print(train_images[0].shape)\n",
    "print(train_images[0].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the data it is stored in a numpy array of 28,28 and a single unsigned char data type. We need to transform this data into the correct type for machine learning. In particular we need to convert the data into a Tensor of type float32, then we need to batch the data into a DataLoader.\n",
    "\n",
    "We can use the torchvision library to transform our data as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.float32\n",
      "tensor(0.) tensor(1.)\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "trans = transforms.Compose([transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True)])\n",
    "tensor = trans(train_images[0])\n",
    "print(tensor.shape)\n",
    "print(tensor.dtype)\n",
    "print(tensor.min(), tensor.max())\n",
    "print(tensor.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the data for this tensor is processed on the CPU, we can convert it to run on the GPU by using the .to(device) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.to(device).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data for Training\n",
    "\n",
    "Earlier, we created a `trans` variable to convert our ndarray to a tensor. [Transforms](https://pytorch.org/vision/stable/transforms.html) are a group of torchvision functions that can be used to transform a dataset.\n",
    "\n",
    "At present our train_images and test_images are numpy arrays. We need to convert them to tensors. We can do this using the `trans` variable we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_tensor = torch.tensor(train_images, dtype=torch.float32).to(device)\n",
    "train_labels_tensor = torch.tensor(train_labels, dtype=torch.uint8).to(device)\n",
    "\n",
    "test_images_tensor = torch.tensor(test_images, dtype=torch.float32).to(device)\n",
    "test_labels_tensor = torch.tensor(test_labels, dtype=torch.uint8).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders\n",
    "\n",
    "We can use the DataLoader class from the torch.utils.data module to create a DataLoader for our training and test data, you can think of this as batching images into smaller groups for training.\n",
    "\n",
    "First we need to create a custom class to hold our data, we can do this by creating a subclass of the Dataset class from the torch.utils.data module. We need to implement the __len__ and __getitem__ methods to return the length of the dataset and the data and label for a given index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class DigitsDataset(Dataset):\n",
    "    def __init__(self, images_tensor, labels_tensor):\n",
    "        self.images_tensor = images_tensor\n",
    "        self.labels_tensor = labels_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images_tensor[idx]\n",
    "        label = self.labels_tensor[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We could show our models the entire dataset at once. Not only does this take a lot of computational resources, but [research shows](https://arxiv.org/pdf/1804.07612) using a smaller batch of data is more efficient for model training.\n",
    "\n",
    "For example, if our `batch_size` is 32, we will train our model by shuffling the deck and drawing 32 cards. We do not need to shuffle for validation as the model is not learning, but we will still use a `batch_size` to prevent memory errors.\n",
    "\n",
    "The batch size is something the model developer decides, and the best value will depend on the problem being solved. Research shows 32 or 64 is sufficient for many machine learning problems and is the default in some machine learning frameworks, so we will use 32 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_data = DigitsDataset(train_images_tensor, train_labels_tensor)\n",
    "valid_data = DigitsDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model\n",
    "\n",
    "Neural networks are composed of layers where each layer performs a mathematical operation on the data it receives before passing it to the next layer. To start, we will create a \"Hello World\" level model made from 4 components:\n",
    "\n",
    "1. A [Flatten](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) used to convert n-dimensional data into a vector. \n",
    "2. An input layer, the first layer of neurons\n",
    "3. A hidden layer, another layer of neurons \"hidden\" between the input and output\n",
    "4. An output layer, the last set of neurons which returns the final prediction from the model\n",
    "\n",
    "We will use a variable called layers to store the layers of our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the image\n",
    "\n",
    "The first thing we need to do is convert the image from a 28x28  array into a flat 1d tensor. We saw the images had 3 dimensions: `C x H x W`. To flatten an image means to combine all of these images into 1 dimension. Let's say we have a tensor like the one below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "test_matrix = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(test_matrix)\n",
    "print(nn.Flatten()(test_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice nothing happened, this is because neural networks expect to receive a batch of data. Currently, the Flatten layer sees three vectors as opposed to one 2d matrix. To fix this, we can \"batch\" our data by adding an extra pair of brackets. Since `test_matrix` is now a tensor, we can do that with the shorthand below. `None` adds a new dimension where `:` selects all the data in a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]]])\n",
      "tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "batch_test_matrix = test_matrix[None, :]\n",
    "print(batch_test_matrix)\n",
    "print(nn.Flatten()(batch_test_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Input Layer\n",
    "\n",
    "The input layer is the first layer of neurons in the neural network. It is responsible for receiving the input data and passing it to the next layer. \n",
    "\n",
    "This layer will be *densely connected*, meaning that each neuron in it, and its weights, will affect every neuron in the next layer.\n",
    "\n",
    "In order to create these weights, Pytorch needs to know the size of our inputs and how many neurons we want to create. \n",
    "Since we've flattened our images, the size of our inputs is the number of channels, number of pixels vertically, and number of pixels horizontally multiplied together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "input_size = 1 * 28 * 28\n",
    "print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the correct number of neurons is what puts the \"science\" in \"data science\" as it is a matter of capturing the statistical complexity of the dataset. For now, we will use `512` neurons. Try playing around with this value later to see how it affects training and to start developing a sense for what this number means.\n",
    "\n",
    "We will learn more about activation functions later, but for now, we will use the [relu](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html) activation function, which in short, will help our network to learn how to make more sophisticated guesses about data than if it were required to make guesses based on some strictly linear function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flatten(start_dim=1, end_dim=-1),\n",
       " Linear(in_features=784, out_features=512, bias=True),\n",
       " ReLU()]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [nn.Flatten(), nn.Linear(input_size, 512), nn.ReLU()]  # Input  # Activation for input\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The hidden layer\n",
    "\n",
    "A hidden layer is a layer of neurons between the input and output layers. It is called \"hidden\" because it is not directly exposed to the input data and the output predictions. We will cover why we combine multiple layers in another lecture, but for now, we will add a hidden layer to our model.\n",
    "\n",
    "As with the previous layers, the shape of the data is important. [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) needs to know the shape of the data being passed to it. Each neuron in the previous layer will compute one number, so the number of inputs into the hidden layer is the same as the number of neurons in the previous later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flatten(start_dim=1, end_dim=-1),\n",
       " Linear(in_features=784, out_features=512, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=512, out_features=512, bias=True),\n",
       " ReLU()]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = [\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_size, 512),  # Input\n",
    "    nn.ReLU(),  # Activation for input\n",
    "    nn.Linear(512, 512),  # Hidden\n",
    "    nn.ReLU(),  # Activation for hidden\n",
    "]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Output Layer\n",
    "\n",
    "The output layer is the final layer of neurons in the neural network. It is responsible for producing the output of the model. This will be a tensor of length 10, where each element represents the probability of the input image being a particular digit.\n",
    "\n",
    "We will not assign the `relu` function to the output layer. Instead, we will apply a `loss function` covered in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Flatten(start_dim=1, end_dim=-1),\n",
       " Linear(in_features=784, out_features=512, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=512, out_features=512, bias=True),\n",
       " ReLU(),\n",
       " Linear(in_features=512, out_features=10, bias=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "\n",
    "layers = [\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(input_size, 512),  # Input\n",
    "    nn.ReLU(),  # Activation for input\n",
    "    nn.Linear(512, 512),  # Hidden\n",
    "    nn.ReLU(),  # Activation for hidden\n",
    "    nn.Linear(512, n_classes),  # Output\n",
    "]\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "\n",
    "A [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) model expects a sequence of arguments, not a list, so we can use the [* operator](https://docs.python.org/3/reference/expressions.html#expression-lists) to unpack our list of layers into a sequence. We can print the model to verify these layers loaded correctly. We can also send our model to the GPU using the to(device) method. We can verify where the model is by using the .device attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(*layers)\n",
    "model.to(device)\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch 2.0 introduced the ability to [compile]([here](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html).) the model using the `compile` method which can give faster performance.  However it does not work with mps backend. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Now that we have prepared training and validation data, and a model, it's time to train our model with our training data, and verify it with its validation data.\n",
    "\n",
    "This is called fitting and we need to add two functions to help us with this process.\n",
    "\n",
    "## Loss and Optimization\n",
    "\n",
    "The loss function measures the difference between the model's prediction and the target. The optimizer updates the model's parameters to reduce the loss. In this example we will use  a loss function called [CrossEntropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) which is designed to grade if a model predicted the correct category from a group of categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer is used to update the model's weights based on the data it sees and the loss function. We will use the [Adam](https://pytorch.org/docs/stable/optim.html) optimizer, which is a popular optimizer in deep learning. It needs to know the models parameters so it can update them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "When training a model, it is important to know how well it is performing. We can calculate the accuracy of the model by comparing the model's prediction to the actual target. The simplest way to do this is to compare the model's prediction to the target and calculate the percentage of correct predictions. \n",
    "\n",
    "We need to generate our own accuracy functions as these are typically dependent on the problem being solved.\n",
    "\n",
    "We need to compare the number of correct classifications compared to the total number of predictions made. Since we're showing data to the model in batches, our accuracy can be calculated along with these batches.\n",
    "\n",
    "We typically use N as a postfix to denote the number of samples in a dataset. We can use this to calculate the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_N = len(train_loader.dataset)\n",
    "valid_N = len(valid_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then accumulate the accuracy for each batch and divide by the total number of samples to get the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_accuracy(output, y, N):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(y.view_as(pred)).sum().item()\n",
    "    return correct / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training Loop\n",
    "\n",
    "We now generate a function that will train the model for a single epoch. This is similar to the approch we used in the the Linear example but we have added a few more steps to calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    # put model into training mode\n",
    "    model.train()\n",
    "    # send data to the device\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = loss_function(output, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += get_batch_accuracy(output, y, train_N)\n",
    "    print(\"Train - Loss: {:.4f} Accuracy: {:.4f}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Once the model has done a training step we need to see how close we are to the correct answer. We can do this by running the model on the validation data and calculating the loss and accuracy of the model on the validation data. Agin we will use a function to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate():\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    # put model into evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "\n",
    "            loss += loss_function(output, y).item()\n",
    "            accuracy += get_batch_accuracy(output, y, valid_N)\n",
    "    print(\"Valid - Loss: {:.4f} Accuracy: {:.4f}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Training loop\n",
    "\n",
    "We can now create a training loop that will train the model for a number of epochs. An `epoch` is one complete pass through the entire dataset. Let's train and validate the model for 5 `epochs` to see how it learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Train - Loss: 639.9409 Accuracy: 0.9217\n",
      "Valid - Loss: 57.4882 Accuracy: 0.9498\n",
      "Epoch: 1\n",
      "Train - Loss: 341.3024 Accuracy: 0.9512\n",
      "Valid - Loss: 62.9680 Accuracy: 0.9471\n",
      "Epoch: 2\n",
      "Train - Loss: 278.1897 Accuracy: 0.9598\n",
      "Valid - Loss: 49.6020 Accuracy: 0.9568\n",
      "Epoch: 3\n",
      "Train - Loss: 245.2574 Accuracy: 0.9654\n",
      "Valid - Loss: 50.5998 Accuracy: 0.9629\n",
      "Epoch: 4\n",
      "Train - Loss: 220.6764 Accuracy: 0.9696\n",
      "Valid - Loss: 46.5469 Accuracy: 0.9665\n",
      "Epoch: 5\n",
      "Train - Loss: 185.5318 Accuracy: 0.9729\n",
      "Valid - Loss: 60.5998 Accuracy: 0.9582\n",
      "Epoch: 6\n",
      "Train - Loss: 192.8441 Accuracy: 0.9744\n",
      "Valid - Loss: 65.3466 Accuracy: 0.9604\n",
      "Epoch: 7\n",
      "Train - Loss: 182.3354 Accuracy: 0.9773\n",
      "Valid - Loss: 37.6866 Accuracy: 0.9734\n",
      "Epoch: 8\n",
      "Train - Loss: 163.7126 Accuracy: 0.9797\n",
      "Valid - Loss: 45.9748 Accuracy: 0.9703\n",
      "Epoch: 9\n",
      "Train - Loss: 157.2278 Accuracy: 0.9793\n",
      "Valid - Loss: 56.8652 Accuracy: 0.9630\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    train()\n",
    "    validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we are quite close (nearly 100%) so we can try and test our model on some existing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(test_images_tensor[0].to(device).unsqueeze(0))\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be ten numbers, each corresponding to a different output neuron. Thanks to how the data is structured, the index of each number matches the corresponding handwritten number. The 0th index is a prediction for a handwritten 0, the 1st index is a prediction for a handwritten 1, and so on.\n",
    "\n",
    "We can use the `argmax` function to find the index of the highest value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7]], device='mps:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(prediction.argmax(dim=1, keepdim=True))\n",
    "display_image(test_images[0], prediction.argmax(dim=1, keepdim=True).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to have worked, we should now save the model so we can use it later.\n",
    "\n",
    "## Saving the Model\n",
    "\n",
    "We can save the model using the torch.save function. We can save the model to a file called `digits.pth` in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAAB1CAYAAABeQY8uAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAPnklEQVR4nO2dX2xT5f/H3+f0nJ6es/5du3VlG+s2JuNPlD+SKFwQNQoaEmI0muiFCCYYNdzgnTGoxMQLr1FipqKgEiKJf0JMFNCEyMUgBAiB8GfrVmDt2q3tetqenj99fhfkPN+NMdlg6073O6+kCWvPefq0L57/n+cpQwghsJlX2PnOgI0twRLYEiyALcEC2BIsgC3BAtgSLIAtwQLYEizAA0n49ttvwTAMzpw5MyuZYBgG77333qykNT7Njz76aNbS+/vvv8EwzJSPt99++4HT5mYtlwucNWvW4PTp05Oe/+KLL/Ddd9/hxRdffOC0bQnTxOv14oknnpjwHCEEr7/+Otra2vDss88+cNpz1iYoioLdu3dj1apV8Pl8qK+vx5NPPolffvllynv279+PRx55BIIgYPny5fjpp58mXZNIJLBz5060tLTA6XSivb0dH3/8MXRdn6uPMiUnT55EX18f3nzzTbDsQ3yV5AH45ptvCADS29s75TXZbJZs27aNfP/99+TEiRPkjz/+IO+//z5hWZYcOHBgwrUASGtrK1m+fDn58ccfya+//ko2b95MAJAjR47Q64aGhkhraytpa2sj+/fvJ3/99RfZu3cvEQSBbNu2bVKae/bsue9neeONNwgA0t/fP6PvgBBCXnvtNcKyLBkYGJjxveOZMwl3o+s60TSN7Nixg6xevXpiJgAiiiJJJBITru/u7iZLliyhz+3cuZO43e5JH/rzzz8nAMilS5cmpDkdCdu3bycOh4PEYrFpfxZCCMlkMsTlcpFNmzbN6L57Madd1CNHjmDDhg1wu93gOA48z6OnpweXL1+edO0zzzyDcDhM/3Y4HHj11Vdx/fp13Lx5EwDw+++/46mnnsKiRYug6zp9PP/88wCAf/75Z8Z57Onpga7raGtrm9F9hw4dgqIoeOutt2b8nnczZxKOHj2KV155Bc3NzTh48CBOnz6N3t5ebN++HYqiTLq+qalpyudGRkYAAMlkEr/99ht4np/wWLFiBQAgnU7P1ceZRE9PDxoaGrB169aHTmvOekcHDx5Ee3s7Dh8+DIZh6PPlcvme1ycSiSmfCwaDAIBQKIRHH30Un3766T3TWLRo0cNme1qcO3cO586dw+7du8Hz/EOnN2cSGIaB0+mcICCRSEzZOzp+/DiSySStkgzDwOHDh9HZ2YmWlhYAwJYtW3Ds2DF0dnYiEAjMVdbvS09PDwBgx44ds5LeQ0k4ceIEYrHYpOdfeOEFbNmyBUePHsU777yDl19+GfF4HHv37kUkEsG1a9cm3RMKhfD000/jww8/RF1dHfbt24crV65M6KZ+8skn+PPPP7F+/Xrs2rULS5cuhaIoiMViOHbsGL788ksqbLrs2LEDBw4cwI0bN6bVLiiKgh9++AHr16/HsmXLZvReU/IgrbnZO5rqYXb3PvvsMxKNRokgCGTZsmXkq6++Inv27CF3vy0A8u6775J9+/aRzs5OwvM86e7uJocOHZr03qlUiuzatYu0t7cTnudJfX09Wbt2Lfnggw+ILMsT0pyLLuqhQ4cIAPL1119P6/rpwBBiR1vMN/YsqgWwJVgAW4IFsCVYAFuCBbAlWABbggWY9oh5/PSDzfSY7hDMLgkWwJZgAWwJFsCWYAFsCRbAlmABbAkWwJZgAWwJFsCWYAFsCRbAlmABbAkWwJZgAWwJFsCWYAFsCRaganvWzDB2lmXhdDrB8zwIIahUKgD+twpFCIGmadA0jb5O7mxmof9eaFRFAsuyaGxsRCQSgSRJ6OjoQDgchq7rKBaLMAwDhBDoug7DMHDr1i3cunULuq5DURRomgZVVSHLMjRNq0aWq0rVJPh8PrS0tMDv92PdunXo7OyEqqrI5XLQNA2GYUDXdWiaBkmSoOs6yuUyZFmGoigolUooFovVyG7VqYoEhmHg9/vR1tYGv9+PxsZGBAIB6LoOl8sFTdNQqVSoCFVVwXEcVFVFoVBAuVxGqVRCKpW65y6fh6FSqUDXdRBCUCwWMTY2RvNQrVJXFQkcx2Hp0qXYvHkzfD4fmpqaEAgEQAiBYRgT2gVCCFavXo1isYhKpQJFUaCqKkqlEpLJJEql0qzmTdM0yLIMVVXR19eHCxcuIJ/PY3R0FCMjI1Vpg6pWEszqyOv1IhAIoK6uDgzDTPiQd4fVVCoV+j+yWCyisbGRSjDvm2kozt33lctl5HI5lMtl6LqOWCwGQghkWZ6Uv7miKhIMw0A8Hkdvby/cbjdCoRA8Hg9teA3DAM/zcLlccDgcEAQBoijC4XDA5XKB53kqsq6ujlYXwJ1eF8dxU8oY3+syqzuGYcDzPBwOB3Rdh8fjoSUiGo0ik8mgWCwimUxW4+upnoQrV66gUqlAkiTaJpRKJYyMjKBUKsHr9SIYDEIQBASDQYRCIYiiiJaWFoRCIXAch1AoBJ7nUSwWkc/nUalU4Ha74Xa7/1NCpVKhVZuiKGBZFnV1dXC5XBNeZ1kWuVwO6XQa2WwWN27cgGEYc/79VEUCIQSFQgGpVAqiKIJlWWiahlKphOHhYSiKQruqgiCgUqmAYRhIkgSv1wuXywVBEOB0OsGyLC0JZqOq6/qUxxqYY4tKpQJN01Aul2nJMUuYiSiKEEURLpcLHFe9Yz+q8k6VSgXZbBaEEPA8j1QqRXtFhUIBmqbB5XIhHo+DZVm43W7U1dXB6XQiFArB5/NBkiRaOnK5HFKpFAzDQDAYRDAYnLIkmL2uSqWCsbEx5PN5eL1ebNy4EStWrADLsnQQqWkaxsbGMDY2Nuu9sP+iaiUhl8thbGyMng80/jWT8a8xDAOHwwFJkiAIAiRJQnNzMyRJwujoKG7fvg1d1xEOhxGJRKYsCWY7YBgGMpkMMpkMIpEImpqa0NHRQdsU4E5PKZ/PI5fLQVGUqo3Oq1bmzO7nTDCFmOMIsxeTy+UgyzIMw0A+n4fL5frP6kjXddomjJ/6MO8xJZVKJciyjHw+X9WRuaXPOyKEQFXVCaNpjuPo4K1SqSCVSkGW5f9MgxAChmFQX1+PxYsXIxKJoL6+HqIoQtM0ZDIZqKqKgYEBXLp0CclkEslkko5f5hpLSwDuVCeGYUBV1XtOW5j1+P0we1fmw+12w+l0Qtd1FAoFFAoFDA8PY3BwEMlkkpaaamB5CQ+L2baYjXxHRwcaGxvh8XgAAKqqIpFIIJvNIp1OQ1EUWj1ViwUvged5iKIISZLw2GOPYevWrXT0bo4Lzpw5g/7+fly9ehXZbJZOmVSLBS/B7IIKgoBQKIRoNEoHeMCdaYvh4WHE43Gk02mUy+WqlgLg/4EEr9eLaDQKv9+PcDgMURThdDrpvJSiKLRHVCqV5mXRaMFLCIVCWL16NUKhENrb2+HxeMDzPJ2ZlWUZo6OjSKfTyOfzVS8FwAKWwLIsGIaBKIrw+/2or6+HJElwOBwAQCWUSiU6p2SuK1SbBSnB6XQiEAjA5XKhq6sLjz/+OBobG9HS0gKHw4FCoYDe3l5cu3YNg4OD6O/vx8jISFW7peNZkBIEQUBDQwPcbje6urqwdu1ahMNhcBwHlmVRLBZx9uxZnDx5EtlsFn19fcjlcgCmv+11NlmQIS8sy8LlcsHj8dC5JzO6w1yvlmWZTn+Y1dB8RXIsyJIgiiLa2toQiUQQiUQgCAIYhkE2m8XIyAiGhoYQi8UQj8dRLpenPByxWixICeboOBKJIBAI0FnSQqFA54VSqRTS6TSd6p5PFpQEcyrc6XROWss2DAPZbBbxeBzJZBKyLNMVtfkOKFtQEhwOB10Uikaj6O7uRkNDA1iWhaqquHr1Ko4fP47R0VHcunWLBp3NNwtGAsMwYFmWBgp4PB74/X5IkgTgzmxsLpdDPB5HLpdDoVCwhABgAUngeR6LFi1CMBhEV1cXwuEwAoEAGIahwQRmWyDL8rw3xuNZMBJcLhe6u7vR3d2NaDSK9vZ2RCIRpFIpDA4OIpvNIhaLYXBwEKVSad4b4/HUvASzGuJ5nobN+P1+iKIInudRqVToBF2xWES5XLZcUHFNSzCXLAOBAEKhENasWYN169bRMBlVVTE0NISzZ88ilUohHo/Py7TE/ahpCSzLIhAIIBqNIhKJYNWqVfR3bwzDQLlcphISiQRu3rxpqWrIpGanLcyekNvtRkNDA0KhEI1VMrukxWIRxWIRsizToF8rUpMlgeM4OJ1OiKKIlStXYtOmTfD7/WhtbQXLslAUBYODg8jlcrh+/ToGBgaQSqVohIbVqEkJ5pYrURTR2tqKVatW0REywzDQNA3pdBqpVAqJRAKpVIr+GokVqTkJDMMgEAjQDSeRSIRWQ2aAVz6fx+3btzE0NISRkZF5+fmvmVBTEszuaFdXF1566SWEw2EsXbqUTk0UCgXk83nEYjGcOnUK165dozFEVqbmJJhbr5YsWYLm5ma6eG/OhpolYWhoCPF4nIZLWpmakeBwOOgAzOfzIRgM0lBGhmFQLpfR39+PgYEB9PX10emJ+VqynAk1I4HjOHi9XoiiiIaGBrpg43A4wDAMSqUSzp8/j9OnT9OpitHRUTpdbWVqQoK5oUOSJLp3wVyyNONUFUVBNptFKpVCJpOh0RO1gKUlmCErPM+jubkZGzduREtLC1auXAlJkmAYBmKxGGKxGFKpFC5cuIC+vj4Ui0XLN8bjsbQElmVpHGlbWxuee+45LF++nC7gmxJOnTqF4eFhXLx4ETdu3KCbBGuFmpDg9Xrh8XjgdrshSRJcLhfd3qooCsbGxlAoFOheBqss1kwXS0sQBAEdHR3o6OhAZ2cnmpqa4Pf76R4zQghdJ8hms5BlueYEABaXwHEcwuEwOjs7sXjxYvj9ftTV1dHXx+8KzWazNdUOjMeSEsztsuY5GM3NzQiFQnA6nfOdtTnBchLMNQIzbmjNmjXYsGEDJEmiu2sWGpaTANwpCV6vFz6fjw7MzC1PCxHLSTCPPGhoaEAwGERdXR04jqMjYwD0LKRCoYBSqUR39Vt9ZDwVlpPAMAy8Xi8WL16MxsZG+Hw++rvO409mMXtD5vyQLWGW4TgOgiBAEAQaVQf8b0+yeRSbefyBWRJqsXsKWFTCvTCPPFBVFZcvX8a///6L0dFRXLx4kW74s+oa8v2oGQmqqiKTyaBQKOD8+fP4+eefkUwmUSgU6CDNLgmziKIoyOVy4DgOt2/fhs/no2GM5s778cFctdoWmEz7l8mr9YuDLMvS43NcLhcikQj8fj89AETXdQwNDaGvr4+eGmbVybrplkzLSbjfe9dSlTPdvFqyOrqbWvriH4RpS1joX8R8UrNhkAsJW4IFsCVYAFuCBbAlWABbggWwJVgAW4IFsCVYgP8DyjMmkOSeRu4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), \"mnist_model.pth\")\n",
    "# now re-load the model into a new model to verify\n",
    "\n",
    "model2 = nn.Sequential(*layers)\n",
    "model2.load_state_dict(torch.load(\"mnist_model.pth\"))\n",
    "model2.to(device)\n",
    "model2.eval()\n",
    "prediction = model2(test_images_tensor[0].to(device).unsqueeze(0))\n",
    "torch.save(test_images_tensor[0], \"test_image.pth\")\n",
    "display_image(test_images[0], prediction.argmax(dim=1, keepdim=True).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above case we saved only the model state dictionary. We can also save the entire model including the architecture and the state dictionary. This is a little more rigid as it requires the model to be defined in the same way when it is loaded, however we don't also need to re-create the model architecture when we load the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.save(model, \"minst_model_full.pth\")\n",
    "model3 = torch.load(\"minst_model_full.pth\")\n",
    "model3.to(device)\n",
    "model3.eval()\n",
    "prediction = model3(test_images_tensor[0].to(device).unsqueeze(0))\n",
    "display_image(test_images[0], prediction.argmax(dim=1, keepdim=True).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use this model to classify new images of digits. We will demonstrate this in a stand alone Qt applications.\n",
    "\n",
    "## Clearing the GPU\n",
    "\n",
    "Sometimes it is best to clear the GPU memory after training a model. There are a number of ways to do this but we can use IPython to re-start the kernel which will clear the GPU memory. There is a function in the Utils module that will do this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "Utils.shutdown_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we have trained a neural network to recognize handwritten digits. We have used the MNIST dataset to train the model and have used a simple neural network with a single hidden layer to classify the images. We have trained the model for 5 epochs and have achieved an accuracy of nearly 100%. We have saved the model so we can use it later.\n",
    "\n",
    "This is basically the process we will use for all of our machine learning models. We will load the data, create a model, train the model and then save the model. We can then use the model to classify new data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
